{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import math as math\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "get_ipython().run_line_magic('matplotlib', 'inline')\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import animation, rc\n",
    "from IPython.display import HTML\n",
    "import ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "c:\\users\\benson\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "c:\\users\\benson\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "c:\\users\\benson\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "c:\\users\\benson\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "c:\\users\\benson\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "c:\\users\\benson\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "c:\\users\\benson\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "c:\\users\\benson\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "c:\\users\\benson\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "c:\\users\\benson\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "c:\\users\\benson\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "c:\\users\\benson\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "folderpath = \"*\"\n",
    "folderpath = \"C:\\\\Users\\\\Benson\\\\Desktop\"\n",
    "file = folderpath + \"\\\\lyftlong\\\\rand_agents_table0.csv\"\n",
    "at = pd.read_csv(file)\n",
    "file = folderpath + \"\\\\lyftlong\\\\rand_frames_table1.csv\"\n",
    "ft = pd.read_csv(file)\n",
    "file = folderpath +  \"\\\\lyftlong\\\\rand_scenes_table1.csv\"\n",
    "st = pd.read_csv(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ft_columns = ['timestamp',\n",
    " 'agent_index_interval_start',\n",
    " 'agent_index_interval_end',\n",
    " 'ego_translation_x',\n",
    " 'ego_translation_y',\n",
    " 'ego_translation_z',\n",
    " 'traffic_lights_start',\n",
    " 'traffic_lights_end',\n",
    " 'ego_rotation_xx',\n",
    " 'ego_rotation_xy',\n",
    " 'ego_rotation_xz',\n",
    " 'ego_rotation_yx',\n",
    " 'ego_rotation_yy',\n",
    " 'ego_rotation_yz',\n",
    " 'ego_rotation_zx',\n",
    " 'ego_rotation_zy',\n",
    " 'ego_rotation_zz',\n",
    " 'frame_index',\n",
    " 'scene_index',\n",
    " 'Mean Distance from Agents']\n",
    "try:\n",
    "    assert list(ft.columns) == ft_columns\n",
    "except:\n",
    "    execute_frametable_column_preparation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Frames_Column_Creator (dict):\n",
    "    def __getitem__(self, attr):\n",
    "        if attr == 'scene_index':\n",
    "            return self.create_frames_scene_index\n",
    "        if attr == 'frame_index':\n",
    "            return self.create_frames_frame_index\n",
    "        if attr == 'Mean Distance from Agents':\n",
    "            return self.create_frames_mean_distance_agents\n",
    "    def create_frames_frame_index(self, st = st):\n",
    "        frame_indexer = []\n",
    "        for i in range(len(st)):\n",
    "            row = st.iloc[i]\n",
    "            frame_indices = list(range(row['frame_index_interval_start'], row['frame_index_interval_end']))\n",
    "            frame_indexer = frame_indexer + frame_indices\n",
    "        ft['frame_index'] = frame_indexer\n",
    "        return frame_indexer.values\n",
    "    def create_frames_scene_index(self, st = st):\n",
    "        scene_indexer = []\n",
    "        for i in range(len(st)):\n",
    "            row = st.iloc[i]\n",
    "            amount = row['frame_index_interval_end'] - row['frame_index_interval_start']\n",
    "            sublst = [i for _ in range(amount)]\n",
    "            scene_indexer = scene_indexer + sublst\n",
    "        ft['scene_index'] = scene_indexer\n",
    "        return scene_indexer.values\n",
    "    def create_frames_mean_distance_agents(self, at = at):\n",
    "        at_temp = at[['centroid_x','centroid_y','frame_index']]\n",
    "        ft_temp = ft[['ego_translation_x', 'ego_translation_y', 'frame_index']]\n",
    "        at_temp = at_temp.merge(ft_temp, on=\"frame_index\")\n",
    "        dx = (at_temp['centroid_x'] - at_temp['ego_translation_x'])**2\n",
    "        dy = (at_temp['centroid_y'] - at_temp['ego_translation_y'])**2\n",
    "        dist = (dx + dy)**(1/2)\n",
    "        at_temp['dist'] = dist\n",
    "        meandist = at_temp.groupby('frame_index',sort=False).mean()['dist']\n",
    "        ft['Mean Distance from Agents'] = meandist.values\n",
    "        return meandist.values\n",
    "class Agents_Column_Creator (dict):\n",
    "    def __getitem__(self, attr):\n",
    "        if attr == 'size':\n",
    "            return self.create_agents_size\n",
    "        if attr == 'frame_index':\n",
    "            return self.create_agents_frame_index\n",
    "    def create_agents_size(self, at = at):\n",
    "        size = (at['extent_x'] * at['extent_y'])\n",
    "        at['size'] = size.values\n",
    "        return size.values\n",
    "    def create_agents_frame_index(self, at = at):\n",
    "        frame_indexer = []\n",
    "        for i in range(len(ft)):\n",
    "            row = ft.iloc[i]\n",
    "            amount = row['agent_index_interval_end'] - row['agent_index_interval_start']\n",
    "            sublst = [i for _ in range(int(amount))]\n",
    "            frame_indexer = frame_indexer + sublst\n",
    "        at['frame_index'] = frame_indexer\n",
    "        return frame_indexer.values   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames_column_creator = Frames_Column_Creator()\n",
    "agents_column_creator = Agents_Column_Creator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "ft_necessary_columns = ['scene_index', 'frame_index', 'Mean Distance from Agents']\n",
    "for colname in ft_necessary_columns:\n",
    "    if colname in ft.columns:\n",
    "        pass\n",
    "    else:\n",
    "        print(\"creating {0}\".format(colname))\n",
    "        create_column_func = frames_column_creator[colname]\n",
    "        create_column_func()\n",
    "at_necessary_columns = ['frame_index', 'size']\n",
    "for colname in at_necessary_columns:\n",
    "    if colname in at.columns:\n",
    "        pass\n",
    "    else:\n",
    "        print(\"creating {0}\".format(colname))\n",
    "        create_column_func = agents_column_creator[colname]\n",
    "        create_column_func()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing Frame instances\n",
    "* Group together the agents with their frame in agent table, using groupby unsorted\n",
    "* For each group of agents, aggregate the average values of the columns: size, velocity_x, velocity_y, yaw\n",
    "* This should collapse the agent table into a table that's n rows long and 5 columns wide, with n = height of frame table\n",
    "* Append this table to the frame table by merging with frame_index\n",
    "* Call the new merged table, ft2. This new table is now ready to be processed into predictions and targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Table_Aggregation:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def aggregator(self, table: pd.DataFrame = None) -> np.array:\n",
    "        \"\"\" Collapses a table of a certain frame_index in a source table into an array\n",
    "        Args:\n",
    "            table (pd.DataFrame): the subset table of the source table with a certain frame_index\n",
    "        Returns:\n",
    "            np.array: the horizontal aggregation of the columns in the subsettted table\n",
    "        \"\"\"\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent_Aggregation (Table_Aggregation):\n",
    "    def __init__(self, table: pd.DataFrame = at, columns: [str] = []):\n",
    "        super(Agent_Aggregation, self).__init__()\n",
    "        self.table = table # agent_table\n",
    "        self.columns = columns\n",
    "        subset_table = table[columns].copy() #[table.frame_index == frame_index][columns].copy()\n",
    "        self.subset_table = subset_table\n",
    "    def aggregator(self, table: pd.DataFrame = None) -> pd.DataFrame:\n",
    "        if table == None:\n",
    "            table = self.subset_table \n",
    "        table['temp_index'] = table['frame_index']\n",
    "        avgs = table.groupby('temp_index', sort=False).mean()\n",
    "        return avgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_ft2(ft=ft):\n",
    "    nn1 = Agent_Aggregation(table = at,\n",
    "                            columns = [\n",
    "                                \"frame_index\",\n",
    "                                \"size\",\n",
    "                                \"velocity_x\",\n",
    "                                \"velocity_y\",\n",
    "                                \"yaw\"\n",
    "                            ])\n",
    "    nn1_table = nn1.aggregator()\n",
    "    ft2 = ft.merge(nn1_table)\n",
    "    return ft2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "ft2 = create_ft2(ft)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ft2 = ft2.drop([\n",
    "    'timestamp', \n",
    "    'agent_index_interval_start', \n",
    "    'agent_index_interval_end',\n",
    "    'traffic_lights_start', \n",
    "    'traffic_lights_end',\n",
    "    'frame_index'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "FT2_PRE_INPUT_COLUMN_ORDER = ['ego_translation_x',\n",
    " 'ego_translation_y',\n",
    " 'ego_translation_z',\n",
    " 'ego_rotation_xx',\n",
    " 'ego_rotation_xy',\n",
    " 'ego_rotation_xz',\n",
    " 'ego_rotation_yx',\n",
    " 'ego_rotation_yy',\n",
    " 'ego_rotation_yz',\n",
    " 'ego_rotation_zx',\n",
    " 'ego_rotation_zy',\n",
    " 'ego_rotation_zz',\n",
    " 'scene_index',\n",
    " 'Mean Distance from Agents',\n",
    " 'size',\n",
    " 'velocity_x',\n",
    " 'velocity_y',\n",
    " 'yaw']\n",
    "# ft2's columns MUST BE in this order before we convert each row into an 1x24 input for the neural network.\n",
    "if (FT2_PRE_INPUT_COLUMN_ORDER) == (list(ft2.columns)):\n",
    "    # ensure that order is the same\n",
    "    pass\n",
    "else:\n",
    "    # colnames all accounted for, but out of order\n",
    "    ft2 = ft2[FT2_PRE_INPUT_COLUMN_ORDER]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing Frame instances (Input preparation)\n",
    "* For each scene in ft2:\n",
    "* pair up each frame-row with previous frame (so the first frame is usually discarded)\n",
    "* create a Frame instance object, with the prev frame-row, and the discrete angle of the current frame-row\n",
    "* put into dataset array frame_instances, which we will later split apart and split into inputs and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Frame_Instance:\n",
    "    def __init__(self, frame_0_array: np.array, frame_1_transl: np.array):\n",
    "        self.frame_0_array = frame_0_array\n",
    "        self.frame_1_translation = frame_1_transl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def angle_discrete(dx, dy):\n",
    "    half_angle = np.arctan2(dy, dx)\n",
    "    if half_angle < 0:\n",
    "        # print(\"too small,\", half_angle)\n",
    "        theta = (math.pi + half_angle) + math.pi\n",
    "    else:\n",
    "        theta = half_angle\n",
    "    assert theta >= 0\n",
    "    assert theta < 2*math.pi\n",
    "    octer = theta // (math.pi/4)\n",
    "    return int(octer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def frame_encoder(frame_0_row, frame_1_row):\n",
    "    input_ = np.array(frame_0_row)\n",
    "    x0, y0 = frame_0_row[['ego_translation_x', 'ego_translation_y']].values\n",
    "    x1, y1 = frame_1_row[['ego_translation_x', 'ego_translation_y']].values\n",
    "    dx = x1 - x0\n",
    "    dy = y1 - y0\n",
    "    one_hot_encoder = np.zeros(8)\n",
    "    octer = angle_discrete(dx, dy)\n",
    "    one_hot_encoder[octer] = 1\n",
    "    return input_, one_hot_encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 of 100 scenes done\n",
      "10 of 100 scenes done\n",
      "20 of 100 scenes done\n",
      "30 of 100 scenes done\n",
      "40 of 100 scenes done\n",
      "50 of 100 scenes done\n",
      "60 of 100 scenes done\n",
      "70 of 100 scenes done\n",
      "80 of 100 scenes done\n",
      "90 of 100 scenes done\n"
     ]
    }
   ],
   "source": [
    "unique_scenes_indices = pd.unique(ft2['scene_index'])\n",
    "frame_instances = np.array([])\n",
    "\n",
    "for s_i in range(len(unique_scenes_indices)):\n",
    "    scene_index = unique_scenes_indices[s_i]\n",
    "    if s_i % 10 == 0:\n",
    "        print('{0} of 100 scenes done'.format(s_i))\n",
    "    ft2_scene = ft2[ft2.scene_index == scene_index]\n",
    "    for i in range(1, len(ft2_scene)):\n",
    "        frame_1_row = ft2_scene.iloc[i]\n",
    "        frame_0_row = ft2_scene.iloc[i - 1].drop(\"scene_index\")\n",
    "        input_, label = frame_encoder(frame_0_row, frame_1_row)\n",
    "        frame_instance = Frame_Instance(input_, label)\n",
    "        frame_instances = np.append(frame_instances, frame_instance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ego_translation_x</th>\n",
       "      <th>ego_translation_y</th>\n",
       "      <th>ego_translation_z</th>\n",
       "      <th>ego_rotation_xx</th>\n",
       "      <th>ego_rotation_xy</th>\n",
       "      <th>ego_rotation_xz</th>\n",
       "      <th>ego_rotation_yx</th>\n",
       "      <th>ego_rotation_yy</th>\n",
       "      <th>ego_rotation_yz</th>\n",
       "      <th>ego_rotation_zx</th>\n",
       "      <th>ego_rotation_zy</th>\n",
       "      <th>ego_rotation_zz</th>\n",
       "      <th>scene_index</th>\n",
       "      <th>Mean Distance from Agents</th>\n",
       "      <th>size</th>\n",
       "      <th>velocity_x</th>\n",
       "      <th>velocity_y</th>\n",
       "      <th>yaw</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>687.888733</td>\n",
       "      <td>-2161.516602</td>\n",
       "      <td>290.123901</td>\n",
       "      <td>-0.548663</td>\n",
       "      <td>0.829618</td>\n",
       "      <td>-0.103456</td>\n",
       "      <td>-0.831995</td>\n",
       "      <td>-0.553972</td>\n",
       "      <td>-0.029972</td>\n",
       "      <td>-0.082177</td>\n",
       "      <td>0.069631</td>\n",
       "      <td>0.994182</td>\n",
       "      <td>7827</td>\n",
       "      <td>29.661129</td>\n",
       "      <td>3.164270</td>\n",
       "      <td>-0.000697</td>\n",
       "      <td>0.001090</td>\n",
       "      <td>0.514642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>687.285095</td>\n",
       "      <td>-2162.442627</td>\n",
       "      <td>290.042358</td>\n",
       "      <td>-0.548607</td>\n",
       "      <td>0.829808</td>\n",
       "      <td>-0.102218</td>\n",
       "      <td>-0.832065</td>\n",
       "      <td>-0.553844</td>\n",
       "      <td>-0.030399</td>\n",
       "      <td>-0.081839</td>\n",
       "      <td>0.068375</td>\n",
       "      <td>0.994297</td>\n",
       "      <td>7827</td>\n",
       "      <td>29.767024</td>\n",
       "      <td>2.391519</td>\n",
       "      <td>0.206666</td>\n",
       "      <td>0.275784</td>\n",
       "      <td>0.494815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>686.678772</td>\n",
       "      <td>-2163.348145</td>\n",
       "      <td>289.963165</td>\n",
       "      <td>-0.548695</td>\n",
       "      <td>0.830226</td>\n",
       "      <td>-0.098282</td>\n",
       "      <td>-0.831977</td>\n",
       "      <td>-0.553805</td>\n",
       "      <td>-0.033390</td>\n",
       "      <td>-0.082151</td>\n",
       "      <td>0.063448</td>\n",
       "      <td>0.994598</td>\n",
       "      <td>7827</td>\n",
       "      <td>29.749838</td>\n",
       "      <td>2.400336</td>\n",
       "      <td>0.165006</td>\n",
       "      <td>0.284153</td>\n",
       "      <td>0.560013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>686.072205</td>\n",
       "      <td>-2164.247314</td>\n",
       "      <td>289.893311</td>\n",
       "      <td>-0.549262</td>\n",
       "      <td>0.830413</td>\n",
       "      <td>-0.093408</td>\n",
       "      <td>-0.831583</td>\n",
       "      <td>-0.554178</td>\n",
       "      <td>-0.036822</td>\n",
       "      <td>-0.082342</td>\n",
       "      <td>0.057452</td>\n",
       "      <td>0.994947</td>\n",
       "      <td>7827</td>\n",
       "      <td>29.634852</td>\n",
       "      <td>1.986993</td>\n",
       "      <td>0.179429</td>\n",
       "      <td>0.235891</td>\n",
       "      <td>0.505815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>685.482300</td>\n",
       "      <td>-2165.144531</td>\n",
       "      <td>289.814209</td>\n",
       "      <td>-0.548979</td>\n",
       "      <td>0.830953</td>\n",
       "      <td>-0.090218</td>\n",
       "      <td>-0.831735</td>\n",
       "      <td>-0.553773</td>\n",
       "      <td>-0.039398</td>\n",
       "      <td>-0.082698</td>\n",
       "      <td>0.053408</td>\n",
       "      <td>0.995143</td>\n",
       "      <td>7827</td>\n",
       "      <td>28.864490</td>\n",
       "      <td>1.741472</td>\n",
       "      <td>0.157543</td>\n",
       "      <td>0.280815</td>\n",
       "      <td>0.527288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24832</td>\n",
       "      <td>-543.666443</td>\n",
       "      <td>903.855469</td>\n",
       "      <td>271.994934</td>\n",
       "      <td>0.681983</td>\n",
       "      <td>0.730755</td>\n",
       "      <td>-0.029941</td>\n",
       "      <td>-0.731367</td>\n",
       "      <td>0.681338</td>\n",
       "      <td>-0.029678</td>\n",
       "      <td>-0.001287</td>\n",
       "      <td>0.042138</td>\n",
       "      <td>0.999111</td>\n",
       "      <td>15427</td>\n",
       "      <td>41.851427</td>\n",
       "      <td>5.012744</td>\n",
       "      <td>-0.377695</td>\n",
       "      <td>-0.639061</td>\n",
       "      <td>-0.169691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24833</td>\n",
       "      <td>-543.666443</td>\n",
       "      <td>903.855469</td>\n",
       "      <td>271.994934</td>\n",
       "      <td>0.681983</td>\n",
       "      <td>0.730755</td>\n",
       "      <td>-0.029941</td>\n",
       "      <td>-0.731367</td>\n",
       "      <td>0.681338</td>\n",
       "      <td>-0.029678</td>\n",
       "      <td>-0.001287</td>\n",
       "      <td>0.042138</td>\n",
       "      <td>0.999111</td>\n",
       "      <td>15427</td>\n",
       "      <td>43.580053</td>\n",
       "      <td>5.145728</td>\n",
       "      <td>-0.788369</td>\n",
       "      <td>-0.208533</td>\n",
       "      <td>-0.293852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24834</td>\n",
       "      <td>-543.666443</td>\n",
       "      <td>903.855469</td>\n",
       "      <td>271.994934</td>\n",
       "      <td>0.681983</td>\n",
       "      <td>0.730755</td>\n",
       "      <td>-0.029941</td>\n",
       "      <td>-0.731367</td>\n",
       "      <td>0.681338</td>\n",
       "      <td>-0.029678</td>\n",
       "      <td>-0.001287</td>\n",
       "      <td>0.042138</td>\n",
       "      <td>0.999111</td>\n",
       "      <td>15427</td>\n",
       "      <td>42.984665</td>\n",
       "      <td>5.145540</td>\n",
       "      <td>-0.947997</td>\n",
       "      <td>-0.099681</td>\n",
       "      <td>-0.104517</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24835</td>\n",
       "      <td>-543.666443</td>\n",
       "      <td>903.855469</td>\n",
       "      <td>271.994934</td>\n",
       "      <td>0.681983</td>\n",
       "      <td>0.730755</td>\n",
       "      <td>-0.029941</td>\n",
       "      <td>-0.731367</td>\n",
       "      <td>0.681338</td>\n",
       "      <td>-0.029678</td>\n",
       "      <td>-0.001287</td>\n",
       "      <td>0.042138</td>\n",
       "      <td>0.999111</td>\n",
       "      <td>15427</td>\n",
       "      <td>44.383615</td>\n",
       "      <td>4.996338</td>\n",
       "      <td>-0.852352</td>\n",
       "      <td>-0.148939</td>\n",
       "      <td>-0.053815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24836</td>\n",
       "      <td>-543.666443</td>\n",
       "      <td>903.855469</td>\n",
       "      <td>271.994934</td>\n",
       "      <td>0.681983</td>\n",
       "      <td>0.730755</td>\n",
       "      <td>-0.029941</td>\n",
       "      <td>-0.731367</td>\n",
       "      <td>0.681338</td>\n",
       "      <td>-0.029678</td>\n",
       "      <td>-0.001287</td>\n",
       "      <td>0.042138</td>\n",
       "      <td>0.999111</td>\n",
       "      <td>15427</td>\n",
       "      <td>41.813361</td>\n",
       "      <td>5.355492</td>\n",
       "      <td>-0.686369</td>\n",
       "      <td>-0.565036</td>\n",
       "      <td>-0.172039</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>24837 rows × 18 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       ego_translation_x  ego_translation_y  ego_translation_z  \\\n",
       "0             687.888733       -2161.516602         290.123901   \n",
       "1             687.285095       -2162.442627         290.042358   \n",
       "2             686.678772       -2163.348145         289.963165   \n",
       "3             686.072205       -2164.247314         289.893311   \n",
       "4             685.482300       -2165.144531         289.814209   \n",
       "...                  ...                ...                ...   \n",
       "24832        -543.666443         903.855469         271.994934   \n",
       "24833        -543.666443         903.855469         271.994934   \n",
       "24834        -543.666443         903.855469         271.994934   \n",
       "24835        -543.666443         903.855469         271.994934   \n",
       "24836        -543.666443         903.855469         271.994934   \n",
       "\n",
       "       ego_rotation_xx  ego_rotation_xy  ego_rotation_xz  ego_rotation_yx  \\\n",
       "0            -0.548663         0.829618        -0.103456        -0.831995   \n",
       "1            -0.548607         0.829808        -0.102218        -0.832065   \n",
       "2            -0.548695         0.830226        -0.098282        -0.831977   \n",
       "3            -0.549262         0.830413        -0.093408        -0.831583   \n",
       "4            -0.548979         0.830953        -0.090218        -0.831735   \n",
       "...                ...              ...              ...              ...   \n",
       "24832         0.681983         0.730755        -0.029941        -0.731367   \n",
       "24833         0.681983         0.730755        -0.029941        -0.731367   \n",
       "24834         0.681983         0.730755        -0.029941        -0.731367   \n",
       "24835         0.681983         0.730755        -0.029941        -0.731367   \n",
       "24836         0.681983         0.730755        -0.029941        -0.731367   \n",
       "\n",
       "       ego_rotation_yy  ego_rotation_yz  ego_rotation_zx  ego_rotation_zy  \\\n",
       "0            -0.553972        -0.029972        -0.082177         0.069631   \n",
       "1            -0.553844        -0.030399        -0.081839         0.068375   \n",
       "2            -0.553805        -0.033390        -0.082151         0.063448   \n",
       "3            -0.554178        -0.036822        -0.082342         0.057452   \n",
       "4            -0.553773        -0.039398        -0.082698         0.053408   \n",
       "...                ...              ...              ...              ...   \n",
       "24832         0.681338        -0.029678        -0.001287         0.042138   \n",
       "24833         0.681338        -0.029678        -0.001287         0.042138   \n",
       "24834         0.681338        -0.029678        -0.001287         0.042138   \n",
       "24835         0.681338        -0.029678        -0.001287         0.042138   \n",
       "24836         0.681338        -0.029678        -0.001287         0.042138   \n",
       "\n",
       "       ego_rotation_zz  scene_index  Mean Distance from Agents      size  \\\n",
       "0             0.994182         7827                  29.661129  3.164270   \n",
       "1             0.994297         7827                  29.767024  2.391519   \n",
       "2             0.994598         7827                  29.749838  2.400336   \n",
       "3             0.994947         7827                  29.634852  1.986993   \n",
       "4             0.995143         7827                  28.864490  1.741472   \n",
       "...                ...          ...                        ...       ...   \n",
       "24832         0.999111        15427                  41.851427  5.012744   \n",
       "24833         0.999111        15427                  43.580053  5.145728   \n",
       "24834         0.999111        15427                  42.984665  5.145540   \n",
       "24835         0.999111        15427                  44.383615  4.996338   \n",
       "24836         0.999111        15427                  41.813361  5.355492   \n",
       "\n",
       "       velocity_x  velocity_y       yaw  \n",
       "0       -0.000697    0.001090  0.514642  \n",
       "1        0.206666    0.275784  0.494815  \n",
       "2        0.165006    0.284153  0.560013  \n",
       "3        0.179429    0.235891  0.505815  \n",
       "4        0.157543    0.280815  0.527288  \n",
       "...           ...         ...       ...  \n",
       "24832   -0.377695   -0.639061 -0.169691  \n",
       "24833   -0.788369   -0.208533 -0.293852  \n",
       "24834   -0.947997   -0.099681 -0.104517  \n",
       "24835   -0.852352   -0.148939 -0.053815  \n",
       "24836   -0.686369   -0.565036 -0.172039  \n",
       "\n",
       "[24837 rows x 18 columns]"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ft2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(frame_instances) == 24737"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Designating Training and Testing Data\n",
    "* Shuffle the list of samples\n",
    "* Split the first 80% of samples as training dataset, and the last 20% as testing dataset\n",
    "* For each dataset, split them into the x list (input), and the y list (labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_PROPORTION = 0.8\n",
    "# TRAIN_SIZE_INDEXER = int(len(frame_instances) * TRAIN_PROPORTION)\n",
    "\n",
    "def Designate_training_testing_datasets(list_samples: np.ndarray, \n",
    "                                        train_proportion: float = TRAIN_PROPORTION,\n",
    "                                        shuffling: bool = True) -> tuple:\n",
    "    assert train_proportion > 0 and train_proportion < 1\n",
    "    np_frame_instances = list_samples\n",
    "    if shuffling:\n",
    "        np.random.shuffle(np_frame_instances)\n",
    "    \n",
    "    TRAIN_SIZE_INDEXER = int(len(frame_instances) * train_proportion)\n",
    "\n",
    "    Train_Dataset = np_frame_instances[:TRAIN_SIZE_INDEXER]\n",
    "    Test_Dataset = np_frame_instances[TRAIN_SIZE_INDEXER:]\n",
    "    \n",
    "    return (Train_Dataset, Test_Dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Split_dataset_inputs_labels(dataset: np.ndarray) -> np.ndarray:\n",
    "    x = []\n",
    "    y = []\n",
    "    for frame_instance in Train_Dataset:\n",
    "        xi = frame_instance.frame_0_array\n",
    "        x.append(xi)\n",
    "        yi = frame_instance.frame_1_translation\n",
    "        y.append(yi)\n",
    "    return np.array(x), np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "Train_Dataset, Test_Dataset = Designate_training_testing_datasets(frame_instances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x, train_y = Split_dataset_inputs_labels(Train_Dataset)\n",
    "test_x, test_y = Split_dataset_inputs_labels(Test_Dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "np_frame_instances = np.array(frame_instances)\n",
    "np.random.shuffle(np_frame_instances)\n",
    "TRAIN_PROPORTION = 0.8\n",
    "TRAIN_SIZE_INDEXER = int(len(frame_instances) * TRAIN_PROPORTION)\n",
    "Train_Dataset = np_frame_instances[:TRAIN_SIZE_INDEXER]\n",
    "Test_Dataset = np_frame_instances[TRAIN_SIZE_INDEXER:]\n",
    "\n",
    "train_x = []\n",
    "train_y = []\n",
    "for frame_instance in Train_Dataset:\n",
    "    train_xi = frame_instance.frame_0_array\n",
    "    train_x.append(train_xi)\n",
    "    train_yi = frame_instance.frame_1_translation\n",
    "    train_y.append(train_yi)\n",
    "    \n",
    "test_x = []\n",
    "test_y = []\n",
    "for frame_instance in Test_Dataset:\n",
    "    test_xi = frame_instance.frame_0_array\n",
    "    test_x.append(test_xi)\n",
    "    test_yi = frame_instance.frame_1_translation\n",
    "    test_y.append(test_yi)\n",
    "    \n",
    "train_x = np.array(train_x)\n",
    "train_y = np.array(train_y)\n",
    "test_x = np.array(test_x)\n",
    "test_y = np.array(test_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network\n",
    "* You now have train_x, train_y, and test_x, test_y."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential()\n",
    "model.add(Dense(16, input_shape=(17,)))\n",
    "model.add(Dense(14))\n",
    "model.add(Dense(12))\n",
    "model.add(Dense(10))\n",
    "model.add(tf.keras.layers.Dense(8, activation=tf.nn.softmax))\n",
    "model.compile(optimizer=tf.train.AdamOptimizer(),\n",
    "              loss='categorical_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "19789/19789 [==============================] - 1s 45us/sample - loss: 15.5600\n",
      "Epoch 2/10\n",
      "19789/19789 [==============================] - 1s 40us/sample - loss: 2.1918\n",
      "Epoch 3/10\n",
      "19789/19789 [==============================] - 1s 40us/sample - loss: 1.9623\n",
      "Epoch 4/10\n",
      "19789/19789 [==============================] - 1s 40us/sample - loss: 1.5248\n",
      "Epoch 5/10\n",
      "19789/19789 [==============================] - 1s 40us/sample - loss: 1.3841\n",
      "Epoch 6/10\n",
      "19789/19789 [==============================] - 1s 48us/sample - loss: 1.2889\n",
      "Epoch 7/10\n",
      "19789/19789 [==============================] - 1s 41us/sample - loss: 1.2357\n",
      "Epoch 8/10\n",
      "19789/19789 [==============================] - 1s 42us/sample - loss: 1.1832\n",
      "Epoch 9/10\n",
      "19789/19789 [==============================] - 1s 43us/sample - loss: 1.1669\n",
      "Epoch 10/10\n",
      "19789/19789 [==============================] - 1s 40us/sample - loss: 1.0857\n",
      "19789/19789 [==============================] - 0s 24us/sample - loss: 0.7698\n"
     ]
    }
   ],
   "source": [
    "model.fit(train_x, train_y, epochs = 10)\n",
    "test_loss = model.evaluate((test_x), (test_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Note that test_x and test_y can be changed with your own data\n",
    "* To do that, get a frames table of your choice \n",
    "* convert it have the columns of ft2, in the proper order, \n",
    "* run it thru all the cells of  Preparing Frame instances (Input preparation)\n",
    "* then in the process \"Designating training and testing\", immediately skip Designate_training_testing_datasets()\n",
    "* set Testing Dataset as frame_instances, and run it thru Split_dataset_inputs_labels()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
