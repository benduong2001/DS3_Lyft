{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rand Agent Table with RNN for Time Series:\n",
    "\n",
    "MODEL 4\n",
    "\n",
    "\n",
    "* For agents with type Car only\n",
    "* For all scenes \n",
    "* 20 most frequent agents only\n",
    "* zero-padded (front padding included)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at this for more info on multiple parallel series with RNN: https://machinelearningmastery.com/how-to-develop-lstm-models-for-time-series-forecasting/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import keras\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.layers import SimpleRNN\n",
    "from tensorflow.keras.layers import Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folderpath = \"*\"\n",
    "folderpath = \"C:\\\\Users\\\\Benson\\\\Desktop\"\n",
    "file = folderpath + \"\\\\lyftlong\\\\rand_agents_table0.csv\"\n",
    "at = pd.read_csv(file)\n",
    "file = folderpath + \"\\\\lyftlong\\\\rand_frames_table1.csv\"\n",
    "ft = pd.read_csv(file)\n",
    "file = folderpath +  \"\\\\lyftlong\\\\rand_scenes_table1.csv\"\n",
    "st = pd.read_csv(file)\n",
    "\n",
    "at = at.merge(ft[['frame_index', 'scene_index']], on='frame_index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_n_most_frequent(n: int, freq_track_ids):\n",
    "    \"\"\" get's track ids of the n most frequent entities in a scene.\"\"\"\n",
    "    freq_track_ids = freq_track_ids.sort_values('frame_index', ascending=False)\n",
    "    return freq_track_ids['track_id'].iloc[:20].values\n",
    "def make_partitions(ar: np.array, step: int = 3) -> np.array:\n",
    "    len_ar = len(ar)\n",
    "    partitions = np.array([])\n",
    "    for i in range(len_ar - step):\n",
    "        X_partition = ar[i: i + step]\n",
    "        y_partition = ar[i + step]\n",
    "        partition = dict()\n",
    "        partition['X'] = ((X_partition))\n",
    "        partition['y'] = ((y_partition))\n",
    "        partitions = np.append(partitions, partition)\n",
    "    return partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FEATURE: filter to only types with Car; uncomment the above to include all agent types\n",
    "at = at[at.PERCEPTION_LABEL_CAR > 0.5] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "atcols = ['centroid_x', 'centroid_y', 'track_id', 'frame_index', 'scene_index']\n",
    "atrnn = at[atcols]\n",
    "\n",
    "scene_indices = pd.unique(at['scene_index'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "atrnn1s = atrnn[atrnn.scene_index == scene_indices[10]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scene_rnn_datatable(scene_index):\n",
    "    # part 1: getting 20 frequent track_ids\n",
    "    atrnn1s = atrnn[atrnn.scene_index == scene_indices[10]]\n",
    "    track_ids = pd.unique(atrnn1s['track_id'])\n",
    "    freq_track_ids = atrnn1s.groupby(['track_id']).count().reset_index()\n",
    "    NUM_ENTITIES = len(track_ids)\n",
    "    # FEATURE: We will only take the 20 most frequent track ids. Set N_MOST_FREQUENT to NUM_ENTITIES if you don't want that\n",
    "    N_MOST_FREQUENT = 20 # NUM_ENTITIES\n",
    "    most_freq_track_ids = get_n_most_frequent(N_MOST_FREQUENT, freq_track_ids)\n",
    "\n",
    "    # part 2: create sequence mapping\n",
    "    atrnn1s2 = atrnn1s[atrnn1s.track_id.isin(most_freq_track_ids)]\n",
    "    atrnn1s2 = atrnn1s2[['centroid_x','centroid_y', 'track_id', 'frame_index']]\n",
    "    apis = atrnn1s2.pivot(index='frame_index', columns='track_id')\n",
    "    # FEATURE: replace all NaN's with zeros\n",
    "    apis = apis.fillna(0)\n",
    "    \n",
    "    # part 3: partition (list of dictionaries, with x and y)\n",
    "    NSTEPS = 3\n",
    "    apis_arr = apis.to_numpy()\n",
    "    partitions = make_partitions(apis_arr, NSTEPS)\n",
    "    return partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "partitions = []\n",
    "for scene_index in scene_indices:\n",
    "    partition = scene_rnn_datatable(scene_index)\n",
    "    partitions.extend(partition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_PROPORTION = 0.8\n",
    "TRAIN_PROPORTION_INDEXER = int(len(partitions) * 0.8)\n",
    "\n",
    "np.random.shuffle(partitions)\n",
    "\n",
    "X_train = []\n",
    "y_train = []\n",
    "X_test = []\n",
    "y_test = []\n",
    "for i in range(len(partitions)): # [:TRAIN_PROPORTION_INDEXER]:\n",
    "    partition = partitions[i]\n",
    "    if i <= TRAIN_PROPORTION_INDEXER:\n",
    "        X_train.append(partition['X'])\n",
    "        y_train.append(partition['y'])\n",
    "    else:\n",
    "        X_test.append(partition['X'])\n",
    "        y_test.append(partition['y'])        \n",
    "X_train = np.array(X_train)\n",
    "y_train = np.array(y_train)\n",
    "X_test = np.array(X_test)\n",
    "y_test = np.array(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_steps = 3\n",
    "n_features = (40)\n",
    "\n",
    "act = None\n",
    "retseq = True\n",
    "\n",
    "#### define model\n",
    "model = Sequential()\n",
    "model.add(LSTM(100, activation=act, return_sequences=retseq, input_shape=(n_steps, n_features)))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(50, activation=act))\n",
    "model.add(Dense(n_features))\n",
    "model.compile(optimizer='adam', loss='mse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(X_train, y_train, epochs=20, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(history.history['loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_eval = model.evaluate(X_test, y_test, batch_size=100)\n",
    "print(model_eval)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SECTION LIMIT"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
