{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rand Agent Table with RNN for Time Series: for all agents in a given frame, version 1: doing it for all agents in a scene"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The way it works: \n",
    "* it uses the same multiple parallel series process seen in the rnn notebook for single agents\n",
    "* but this time, we prepared the dataset table to be done differently\n",
    "* for ONE GIVEN SCENE, we separate the agent table into their own sub-tables by their track ids, \n",
    "* and left outer join them each side by side in a new table called Apis, on the frame-index column\n",
    "* The rest of Apis are Nan, representing the points in time when a certain track id entity leaves the vision of the ai car.\n",
    "* We finally apply the same parallel input series process\n",
    "\n",
    "* The result is bad, we still need to fit random forests some where.\n",
    "\n",
    "Look at this for more info on multiple parallel series with RNN: https://machinelearningmastery.com/how-to-develop-lstm-models-for-time-series-forecasting/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import keras\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folderpath = \"*\"\n",
    "file = folderpath + \"\\\\lyftlong\\\\rand_agents_table0.csv\"\n",
    "at = pd.read_csv(file)\n",
    "file = folderpath + \"\\\\lyftlong\\\\rand_frames_table1.csv\"\n",
    "ft = pd.read_csv(file)\n",
    "file = folderpath +  \"\\\\lyftlong\\\\rand_scenes_table1.csv\"\n",
    "st = pd.read_csv(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "at = at.merge(ft[['frame_index', 'scene_index']], on='frame_index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "atcols = ['centroid_x', 'centroid_y', 'extent_x', 'extent_y', 'extent_z',\n",
    "          'velocity_x', 'velocity_y', 'yaw', 'track_id','frame_index', 'scene_index']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "atrnn = at[atcols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scene_indices = pd.unique(at['scene_index'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "atrnn1 = atrnn[atrnn.scene_index == scene_indices[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "atrnn1 # scene 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_parallel_input_series = pd.DataFrame()\n",
    "apis = agent_parallel_input_series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PREPARING THE COLUMNS FOR APIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "atrnn1s = atrnn1 #.sort_values(['track_id', 'frame_index'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# making area:\n",
    "atrnn1s['area'] = atrnn1s['extent_x'] * atrnn1s['extent_y']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "track_ids = pd.unique(atrnn1s['track_id'])\n",
    "NUM_ENTITIES = len(track_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apis_cols = ['centroid_x', 'centroid_y', 'area', 'velocity_x', 'velocity_y', 'frame_index']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apis_frame_indices = pd.unique(atrnn1['frame_index'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apis['frame_index'] = apis_frame_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(track_ids)):\n",
    "    if i%100 == 0:\n",
    "        print(\"{0} of {1}\".format(i, len(track_ids)))\n",
    "    ti = track_ids[i]\n",
    "    # for each track id, get it's subset in atrnn1s\n",
    "    atrnn1sti = atrnn1s[atrnn1s.track_id == ti]\n",
    "    # get these columns only\n",
    "    atrnn1sti = atrnn1sti[apis_cols]\n",
    "    # rename the columns by labelling it with it's track id\n",
    "    atrnn1sti = atrnn1sti.add_prefix('ti{0}_'.format(str(i)))\n",
    "    # undo apis index's renaming for joining to apis\n",
    "    atrnn1sti.rename(columns = {\"ti{0}_frame_index\".format(str(i)): \"frame_index\"},  \n",
    "                     inplace = True) \n",
    "    # LEFT OUTER JOIN with APIS, atrnn1sti\n",
    "    apis = pd.merge(apis, atrnn1sti, on='frame_index', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "frame_indexer_col_count = 1\n",
    "assert apis.shape == (248, 5860 + frame_indexer_col_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_partitions(ar: np.array, step: int = 3) -> np.array:\n",
    "    len_ar = len(ar)\n",
    "    partitions = np.array([])\n",
    "    for i in range(len_ar - step):\n",
    "        X_partition = ar[i: i + step]\n",
    "        y_partition = ar[i + step]\n",
    "        partition = dict()\n",
    "        partition['X'] = ((X_partition))\n",
    "        partition['y'] = ((y_partition))\n",
    "        partitions = np.append(partitions, partition)\n",
    "    return partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apis.drop('frame_index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apis = apis.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "atrnn1 = apis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_partitions(ar: np.array, step: int = 3) -> np.array:\n",
    "    len_ar = len(ar)\n",
    "    partitions = np.array([])\n",
    "    for i in range(len_ar - step):\n",
    "        X_partition = ar[i: i + step]\n",
    "        y_partition = ar[i + step]\n",
    "        partition = dict()\n",
    "        partition['X'] = ((X_partition))\n",
    "        partition['y'] = ((y_partition))\n",
    "        partitions = np.append(partitions, partition)\n",
    "    return partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NSTEPS = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "partitions = make_partitions(np.array(atrnn1), NSTEPS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_PROPORTION = 0.8\n",
    "TRAIN_PROPORTION_INDEXER = int(len(atrnn1) * 0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = []\n",
    "y_train = []\n",
    "X_test = []\n",
    "y_test = []\n",
    "for i in range(len(partitions)): # [:TRAIN_PROPORTION_INDEXER]:\n",
    "    partition = partitions[i]\n",
    "    if i <= TRAIN_PROPORTION_INDEXER:\n",
    "        X_train.append(partition['X'])\n",
    "        y_train.append(partition['y'])\n",
    "    else:\n",
    "        X_test.append(partition['X'])\n",
    "        y_test.append(partition['y'])        \n",
    "X_train = np.array(X_train)\n",
    "y_train = np.array(y_train)\n",
    "X_test = np.array(X_test)\n",
    "y_test = np.array(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_steps = NSTEPS\n",
    "n_features = len(atrnn1.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# define model\n",
    "model = Sequential()\n",
    "model.add(LSTM(100, activation='relu', return_sequences=True, input_shape=(n_steps, n_features)))\n",
    "model.add(LSTM(100, activation='relu', return_sequences=True, input_shape=(n_steps, n_features)))\n",
    "model.add(LSTM(100, activation='relu', return_sequences=True, input_shape=(n_steps, n_features)))\n",
    "model.add(LSTM(100, activation='relu'))\n",
    "model.add(Dense(n_features))\n",
    "model.compile(optimizer='adam', loss='mse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.fit(X_train, y_train, epochs=300, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_inp = X_test[0]\n",
    "y_targ = y_test[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_inp = x_inp = x_inp.reshape((1, n_steps, n_features))\n",
    "x_inp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat = model.predict(x_inp, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat # y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_targ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sqrt(((y_hat - y_targ)**2)).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import spatial\n",
    "\n",
    "CosineSimilarity = 1 - spatial.distance.cosine(y_hat, y_targ)\n",
    "CosineSimilarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
