{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rand Agent Table with RNN for Time Series: for all agents in a given frame, version 1: doing it for all agents in a scene"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The way it works: \n",
    "* it uses the same multiple parallel series process seen in the rnn notebook for single agents\n",
    "* but this time, we prepared the dataset table to be done differently\n",
    "* for ONE GIVEN SCENE, we separate the agent table into their own sub-tables by their track ids, \n",
    "* and left outer join them each side by side in a new table called Apis, on the frame-index column\n",
    "* The rest of Apis are Nan, representing the points in time when a certain track id entity leaves the vision of the ai car.\n",
    "* We finally apply the same parallel input series process\n",
    "\n",
    "* The result is bad, we still need to fit random forests some where.\n",
    "\n",
    "Look at this for more info on multiple parallel series with RNN: https://machinelearningmastery.com/how-to-develop-lstm-models-for-time-series-forecasting/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import keras\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folderpath = \"*\"\n",
    "file = folderpath + \"\\\\lyftlong\\\\rand_agents_table0.csv\"\n",
    "at = pd.read_csv(file)\n",
    "file = folderpath + \"\\\\lyftlong\\\\rand_frames_table1.csv\"\n",
    "ft = pd.read_csv(file)\n",
    "file = folderpath +  \"\\\\lyftlong\\\\rand_scenes_table1.csv\"\n",
    "st = pd.read_csv(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "at = at.merge(ft[['frame_index', 'scene_index']], on='frame_index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "atcols = ['centroid_x', 'centroid_y', 'extent_x', 'extent_y', 'extent_z',\n",
    "          'velocity_x', 'velocity_y', 'yaw', 'track_id','frame_index', 'scene_index']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "atrnn = at[atcols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scene_indices = pd.unique(at['scene_index'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "atrnn1 = atrnn[atrnn.scene_index == scene_indices[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "atrnn1['c'] = np.ones(len(atrnn1))\n",
    "atrnn1.groupby('frame_index').count()['c'].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_parallel_input_series = pd.DataFrame()\n",
    "apis = agent_parallel_input_series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PREPARING THE COLUMNS FOR APIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "atrnn1s = atrnn1 #.sort_values(['track_id', 'frame_index'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# making area:\n",
    "atrnn1s['area'] = atrnn1s['extent_x'] * atrnn1s['extent_y']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "track_ids = pd.unique(atrnn1s['track_id'])\n",
    "NUM_ENTITIES = len(track_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apis_cols = ['centroid_x', 'centroid_y', 'area', 'velocity_x', 'velocity_y', 'frame_index']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apis_frame_indices = pd.unique(atrnn1['frame_index'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apis['frame_index'] = apis_frame_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(track_ids)):\n",
    "    if i%100 == 0:\n",
    "        print(\"{0} of {1}\".format(i, len(track_ids)))\n",
    "    ti = track_ids[i]\n",
    "    # for each track id, get it's subset in atrnn1s\n",
    "    atrnn1sti = atrnn1s[atrnn1s.track_id == ti]\n",
    "    # get these columns only\n",
    "    atrnn1sti = atrnn1sti[apis_cols]\n",
    "    # rename the columns by labelling it with it's track id\n",
    "    atrnn1sti = atrnn1sti.add_prefix('ti{0}_'.format(str(i)))\n",
    "    # undo apis index's renaming for joining to apis\n",
    "    atrnn1sti.rename(columns = {\"ti{0}_frame_index\".format(str(i)): \"frame_index\"},  \n",
    "                     inplace = True) \n",
    "    # LEFT OUTER JOIN with APIS, atrnn1sti\n",
    "    apis = pd.merge(apis, atrnn1sti, on='frame_index', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apis.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "frame_indexer_col_count = 1\n",
    "assert apis.shape == (248, 5850 + frame_indexer_col_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_partitions(ar: np.array, step: int = 3) -> np.array:\n",
    "    len_ar = len(ar)\n",
    "    partitions = np.array([])\n",
    "    for i in range(len_ar - step):\n",
    "        X_partition = ar[i: i + step]\n",
    "        y_partition = ar[i + step]\n",
    "        partition = dict()\n",
    "        partition['X'] = ((X_partition))\n",
    "        partition['y'] = ((y_partition))\n",
    "        partitions = np.append(partitions, partition)\n",
    "    return partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "apis = apis.drop(['frame_index'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apis = apis.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "atrnn1 = apis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampcols= ['ti0_centroid_x', 'ti0_centroid_y', 'ti0_area', 'ti0_velocity_x',\n",
    "       'ti0_velocity_y', 'ti1_centroid_x', 'ti1_centroid_y', 'ti1_area',\n",
    "       'ti1_velocity_x', 'ti1_velocity_y']\n",
    "\n",
    "apisrf = apis[sampcols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "apisrf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.pluralsight.com/guides/machine-learning-for-time-series-data-in-python\n",
    "# Random Forest Regressors\n",
    "\n",
    "# https://towardsdatascience.com/how-not-to-use-machine-learning-for-time-series-forecasting-avoiding-the-pitfalls-19f9d7adf424\n",
    "# please look at the cross-correlation to see if data is cross correlated (patternous)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import model_selection\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_PROPORTION = 0.8\n",
    "TRAIN_PROPORTION_INDEXER = int(len(apisrf) * 0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = apisrf.iloc[:TRAIN_PROPORTION_INDEXER]\n",
    "test = apisrf.iloc[TRAIN_PROPORTION_INDEXER:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_column_train = ['ti0_velocity_x'] \n",
    "predictors_train = list(set(list(train.columns))-set(target_column_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train[predictors_train].values\n",
    "y_train = train[target_column_train].values\n",
    "\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_column_test = ['ti0_velocity_x'] \n",
    "predictors_test = list(set(list(test.columns))-set(target_column_test))\n",
    "\n",
    "X_test = test[predictors_test].values\n",
    "y_test = test[target_column_test].values\n",
    "\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_partitions(ar: np.array, step: int = 3) -> np.array:\n",
    "    len_ar = len(ar)\n",
    "    partitions = np.array([])\n",
    "    for i in range(len_ar - step):\n",
    "        X_partition = ar[i: i + step]\n",
    "        y_partition = ar[i + step]\n",
    "        partition = dict()\n",
    "        partition['X'] = ((X_partition))\n",
    "        partition['y'] = ((y_partition))\n",
    "        partitions = np.append(partitions, partition)\n",
    "    return partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_rf = RandomForestRegressor(n_estimators=5000, oob_score=True, random_state=100)\n",
    "model_rf.fit(X_train, y_train) \n",
    "pred_train_rf= model_rf.predict(X_train)\n",
    "print(\"MSE train\", np.sqrt(mean_squared_error(y_train,pred_train_rf)))\n",
    "print(\"R2 train\", r2_score(y_train, pred_train_rf))\n",
    "\n",
    "pred_test_rf = model_rf.predict(X_test)\n",
    "print(\"MSE test\", np.sqrt(mean_squared_error(y_test,pred_test_rf)))\n",
    "print(\"R2 test\", r2_score(y_test, pred_test_rf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "from sklearn.ensemble import GradientBoostingRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = GradientBoostingRegressor()\n",
    "# define the evaluation procedure\n",
    "cv = RepeatedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "# evaluate the model\n",
    "n_scores = cross_val_score(model, X_train, y_train, scoring='neg_mean_absolute_error', cv=cv, n_jobs=-1)\n",
    "# report performance\n",
    "print('MAE: %.3f (%.3f)' % (mean(n_scores), std(n_scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_train, y_train)\n",
    "# make a single prediction\n",
    "# row = [0.20543991, -0.97049844, -0.81403429, -0.23842689, -0.60704084, -0.48541492, 0.53113006, 2.01834338, -0.90745243, -1.85859731, -1.02334791, -0.6877744, 0.60984819, -0.70630121, -1.29161497, 1.32385441, 1.42150747, 1.26567231, 2.56569098, -0.11154792]\n",
    "j = 49\n",
    "\n",
    "yhat = model.predict([X_test[j]])\n",
    "# summarize prediction\n",
    "print('Pred:', yhat[0])\n",
    "print('Targ:', y_test[j][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# xgboost for regression\n",
    "from numpy import asarray\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "from sklearn.datasets import make_regression\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "from matplotlib import pyplot\n",
    "# define dataset\n",
    "X, y = make_regression(n_samples=1000, n_features=10, n_informative=5, random_state=1)\n",
    "# evaluate the model\n",
    "model = XGBRegressor(objective='reg:squarederror')\n",
    "cv = RepeatedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "n_scores = cross_val_score(model, X, y, scoring='neg_mean_absolute_error', cv=cv, n_jobs=-1, error_score='raise')\n",
    "print('MAE: %.3f (%.3f)' % (mean(n_scores), std(n_scores)))\n",
    "# fit the model on the whole dataset\n",
    "model = XGBRegressor(objective='reg:squarederror')\n",
    "model.fit(X, y)\n",
    "# make a single prediction\n",
    "row = [2.02220122, 0.31563495, 0.82797464, -0.30620401, 0.16003707, -1.44411381, 0.87616892, -0.50446586, 0.23009474, 0.76201118]\n",
    "row = asarray(row).reshape((1, len(row)))\n",
    "yhat = model.predict(row)\n",
    "print('Prediction: %.3f' % yhat[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
